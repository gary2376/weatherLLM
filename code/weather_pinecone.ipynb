{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e37ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ¤ï¸  è‡ºä¸­å¸‚å¤©æ°£é å ±å‘é‡åŒ–ç³»çµ±\n",
      "==================================================\n",
      "âœ… å·²åˆå§‹åŒ– Pinecone ç´¢å¼•: weather-txt-index\n",
      "âœ… å·²åˆå§‹åŒ– Pinecone åµŒå…¥æ¨¡å‹: llama-text-embed-v2\n",
      "ğŸ“Š ç´¢å¼•ç‚ºç©ºï¼Œå°‡é–‹å§‹è™•ç†è³‡æ–™\n",
      "âœ… æˆåŠŸæå–è‡ºä¸­å¸‚ 435 ç­†å¤©æ°£è³‡æ–™\n",
      "ğŸ”„ æ­£åœ¨ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ç”ŸæˆåµŒå…¥å‘é‡: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:09<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æˆåŠŸç”Ÿæˆ 435 å€‹åµŒå…¥å‘é‡\n",
      "ğŸ”„ æ­£åœ¨ä¸Šå‚³è³‡æ–™åˆ° Pinecone...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ä¸Šå‚³å‘é‡: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:06<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æˆåŠŸä¸Šå‚³ 435 å€‹å‘é‡åˆ° Pinecone\n",
      "\n",
      "ğŸ¯ å¤©æ°£æŸ¥è©¢ç³»çµ±å·²å•Ÿå‹•ï¼\n",
      "æ‚¨å¯ä»¥è©¢å•é—œæ–¼è‡ºä¸­å¸‚å¤©æ°£çš„å•é¡Œï¼Œä¾‹å¦‚ï¼š\n",
      "- è‡ºä¸­å¸‚ä¸­å€çš„æº«åº¦å¦‚ä½•ï¼Ÿ\n",
      "- ä»Šå¤©è‡ºä¸­æœ€ç†±çš„åœ°å€æ˜¯å“ªè£¡ï¼Ÿ\n",
      "- è‡ºä¸­å¸‚å“ªå€‹å€åŸŸæº«å·®æœ€å¤§ï¼Ÿ\n",
      "è¼¸å…¥ 'quit' é€€å‡º\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31måœ¨ç›®å‰å„²å­˜æ ¼æˆ–ä¸Šä¸€å€‹å„²å­˜æ ¼ä¸­åŸ·è¡Œç¨‹å¼ç¢¼æ™‚ï¼ŒKernel å·²ææ¯€ã€‚\n",
      "\u001b[1;31mè«‹æª¢é–±å„²å­˜æ ¼ä¸­çš„ç¨‹å¼ç¢¼ï¼Œæ‰¾å‡ºå¤±æ•—çš„å¯èƒ½åŸå› ã€‚\n",
      "\u001b[1;31må¦‚éœ€è©³ç´°è³‡è¨Šï¼Œè«‹æŒ‰ä¸€ä¸‹<a href='https://aka.ms/vscodeJupyterKernelCrash'>é€™è£¡</a>ã€‚\n",
      "\u001b[1;31må¦‚éœ€è©³ç´°è³‡æ–™ï¼Œè«‹æª¢è¦– Jupyter <a href='command:jupyter.viewOutput'>è¨˜éŒ„</a>ã€‚"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "å¤©æ°£é å ±è³‡æ–™å‘é‡åŒ–èˆ‡æª¢ç´¢ç³»çµ±\n",
    "\n",
    "ä½¿ç”¨ Pinecone Inference API é€²è¡Œæ–‡æœ¬åµŒå…¥ï¼Œä¸¦å„²å­˜åˆ° Pinecone é€²è¡Œå‘é‡æª¢ç´¢\n",
    "å°ˆæ³¨æ–¼è‡ºä¸­å¸‚å¤©æ°£è³‡æ–™çš„è™•ç†èˆ‡æŸ¥è©¢\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "import json\n",
    "import urllib.parse\n",
    "\n",
    "from pinecone import Pinecone\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "class WeatherVectorizer:\n",
    "    \"\"\"å¤©æ°£è³‡æ–™å‘é‡åŒ–è™•ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, pinecone_api_key: str):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–å‘é‡åŒ–è™•ç†å™¨\n",
    "        \n",
    "        Args:\n",
    "            pinecone_api_key: Pinecone API é‡‘é‘°\n",
    "        \"\"\"\n",
    "        # åˆå§‹åŒ– Pinecone\n",
    "        self.pc = Pinecone(api_key=pinecone_api_key)\n",
    "        self.index_name = \"weather-txt-index\"\n",
    "        self.index = self.pc.Index(self.index_name)\n",
    "        \n",
    "        # ä½¿ç”¨ Pinecone çš„ Inference API\n",
    "        self.embedding_model = \"llama-text-embed-v2\"\n",
    "        \n",
    "        print(f\"âœ… å·²åˆå§‹åŒ– Pinecone ç´¢å¼•: {self.index_name}\")\n",
    "        print(f\"âœ… å·²åˆå§‹åŒ– Pinecone åµŒå…¥æ¨¡å‹: {self.embedding_model}\")\n",
    "    \n",
    "    def extract_taichung_data(self, file_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        å¾å¤©æ°£é å ±æ–‡ä»¶ä¸­æå–è‡ºä¸­å¸‚çš„è³‡æ–™\n",
    "        \n",
    "        Args:\n",
    "            file_path: å¤©æ°£é å ±æ–‡ä»¶è·¯å¾‘\n",
    "            \n",
    "        Returns:\n",
    "            åŒ…å«è‡ºä¸­å¸‚å„å€å¤©æ°£è³‡æ–™çš„åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # è§£æè‡ºä¸­å¸‚å„å€çš„è³‡æ–™\n",
    "            districts_data = self._parse_districts_data(content)\n",
    "            \n",
    "            print(f\"âœ… æˆåŠŸæå–è‡ºä¸­å¸‚ {len(districts_data)} ç­†å¤©æ°£è³‡æ–™\")\n",
    "            return districts_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æå–è‡ºä¸­å¸‚è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _parse_districts_data(self, content: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"è§£æè‡ºä¸­å¸‚å„å€çš„å¤©æ°£è³‡æ–™\"\"\"\n",
    "        districts_data = []\n",
    "        \n",
    "        # æ‰¾åˆ°æ‰€æœ‰åŒ…å«è‡ºä¸­å¸‚å€åŸŸè³‡æ–™çš„æ®µè½\n",
    "        # æ¨¡å¼ï¼šåœ¨ æ™‚é–“ æœŸé–“ï¼Œè‡ºä¸­å¸‚XXå€çš„å¤©æ°£ç‹€æ³å¦‚ä¸‹ï¼š\n",
    "        pattern = r'åœ¨ ([\\d\\-T\\+:]+) è‡³ ([\\d\\-T\\+:]+) æœŸé–“ï¼Œè‡ºä¸­å¸‚(\\w+)çš„å¤©æ°£ç‹€æ³å¦‚ä¸‹ï¼š\\s*\\n\\s*- å¹³å‡æº«åº¦ï¼š([\\d.]+)Â°C\\s*\\n\\s*- æœ€ä½æº«åº¦ï¼š([\\d.]+)Â°C\\s*\\n\\s*- æœ€é«˜æº«åº¦ï¼š([\\d.]+)Â°C'\n",
    "        \n",
    "        matches = re.findall(pattern, content)\n",
    "        \n",
    "        for match in matches:\n",
    "            start_time, end_time, district, avg_temp, min_temp, max_temp = match\n",
    "            \n",
    "            time_range = f\"{start_time} è‡³ {end_time}\"\n",
    "            \n",
    "            weather_data = {\n",
    "                \"district\": district,\n",
    "                \"time_range\": time_range,\n",
    "                \"avg_temp\": float(avg_temp),\n",
    "                \"min_temp\": float(min_temp),\n",
    "                \"max_temp\": float(max_temp),\n",
    "                \"text_description\": self._create_text_description(\n",
    "                    district, time_range, avg_temp, min_temp, max_temp\n",
    "                )\n",
    "            }\n",
    "            districts_data.append(weather_data)\n",
    "        \n",
    "        return districts_data\n",
    "    \n",
    "    def _create_text_description(self, district: str, time_range: str, \n",
    "                                avg_temp: str, min_temp: str, max_temp: str) -> str:\n",
    "        \"\"\"ç‚ºå¤©æ°£è³‡æ–™å‰µå»ºæè¿°æ€§æ–‡æœ¬\"\"\"\n",
    "        # è§£ææ™‚é–“ç¯„åœ\n",
    "        time_parts = time_range.split(\" è‡³ \")\n",
    "        if len(time_parts) == 2:\n",
    "            start_time = time_parts[0].replace(\"T\", \" \").replace(\"+08:00\", \"\")\n",
    "            end_time = time_parts[1].replace(\"T\", \" \").replace(\"+08:00\", \"\")\n",
    "            \n",
    "            description = (\n",
    "                f\"è‡ºä¸­å¸‚{district}åœ¨{start_time}è‡³{end_time}æœŸé–“çš„å¤©æ°£é å ±ï¼š\" +\n",
    "                f\"å¹³å‡æº«åº¦ç‚º{avg_temp}åº¦ï¼Œæœ€ä½æº«åº¦{min_temp}åº¦ï¼Œæœ€é«˜æº«åº¦{max_temp}åº¦ã€‚\"\n",
    "            )\n",
    "        else:\n",
    "            description = (\n",
    "                f\"è‡ºä¸­å¸‚{district}çš„å¤©æ°£é å ±ï¼š\" +\n",
    "                f\"å¹³å‡æº«åº¦ç‚º{avg_temp}åº¦ï¼Œæœ€ä½æº«åº¦{min_temp}åº¦ï¼Œæœ€é«˜æº«åº¦{max_temp}åº¦ã€‚\"\n",
    "            )\n",
    "        \n",
    "        return description\n",
    "    \n",
    "    def create_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        ä½¿ç”¨ Pinecone Inference API å‰µå»ºæ–‡æœ¬åµŒå…¥å‘é‡\n",
    "        \n",
    "        Args:\n",
    "            texts: è¦åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨\n",
    "            \n",
    "        Returns:\n",
    "            åµŒå…¥å‘é‡åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        print(\"ğŸ”„ æ­£åœ¨ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡...\")\n",
    "        \n",
    "        # ä½¿ç”¨ Pinecone Inference API æ‰¹æ¬¡è™•ç†\n",
    "        batch_size = 32  # Pinecone å»ºè­°çš„æ‰¹æ¬¡å¤§å°\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"ç”ŸæˆåµŒå…¥å‘é‡\"):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                # ä½¿ç”¨ Pinecone Inference API\n",
    "                response = self.pc.inference.embed(\n",
    "                    model=self.embedding_model,\n",
    "                    inputs=batch_texts,\n",
    "                    parameters={\"input_type\": \"passage\"}\n",
    "                )\n",
    "                \n",
    "                # æå–åµŒå…¥å‘é‡\n",
    "                for embedding_data in response.data:\n",
    "                    embeddings.append(embedding_data.values)\n",
    "                \n",
    "                # é¿å… API é™åˆ¶\n",
    "                time.sleep(0.1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ç”ŸæˆåµŒå…¥å‘é‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "                # ä½¿ç”¨é›¶å‘é‡ä½œç‚ºå‚™ç”¨\n",
    "                for _ in batch_texts:\n",
    "                    embeddings.append([0.0] * 1024)  # llama-text-embed-v2 çš„ç¶­åº¦\n",
    "        \n",
    "        print(f\"âœ… æˆåŠŸç”Ÿæˆ {len(embeddings)} å€‹åµŒå…¥å‘é‡\")\n",
    "        return embeddings\n",
    "    \n",
    "    def _create_safe_vector_id(self, district: str, index: int) -> str:\n",
    "        \"\"\"\n",
    "        å‰µå»ºç¬¦åˆ ASCII è¦æ±‚çš„å‘é‡ ID\n",
    "        \n",
    "        Args:\n",
    "            district: å€åï¼ˆå¯èƒ½åŒ…å«ä¸­æ–‡ï¼‰\n",
    "            index: ç´¢å¼•ç·¨è™Ÿ\n",
    "            \n",
    "        Returns:\n",
    "            ASCII æ ¼å¼çš„å‘é‡ ID\n",
    "        \"\"\"\n",
    "        # ä½¿ç”¨ URL ç·¨ç¢¼å°‡ä¸­æ–‡è½‰æ›ç‚º ASCII\n",
    "        encoded_district = urllib.parse.quote(district, safe='')\n",
    "        return f\"taichung_{encoded_district}_{index}\"\n",
    "\n",
    "    def upload_to_pinecone(self, weather_data: List[Dict[str, Any]], \n",
    "                          embeddings: List[List[float]]) -> None:\n",
    "        \"\"\"\n",
    "        å°‡å¤©æ°£è³‡æ–™å’ŒåµŒå…¥å‘é‡ä¸Šå‚³åˆ° Pinecone\n",
    "        \n",
    "        Args:\n",
    "            weather_data: å¤©æ°£è³‡æ–™åˆ—è¡¨\n",
    "            embeddings: å°æ‡‰çš„åµŒå…¥å‘é‡åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        print(\"ğŸ”„ æ­£åœ¨ä¸Šå‚³è³‡æ–™åˆ° Pinecone...\")\n",
    "        \n",
    "        vectors_to_upsert = []\n",
    "        for i, (data, embedding) in enumerate(zip(weather_data, embeddings)):\n",
    "            # ä½¿ç”¨ ASCII å®‰å…¨çš„ vector ID\n",
    "            vector_id = self._create_safe_vector_id(data['district'], i)\n",
    "            \n",
    "            metadata = {\n",
    "                \"district\": data[\"district\"],\n",
    "                \"time_range\": data[\"time_range\"],\n",
    "                \"avg_temp\": data[\"avg_temp\"],\n",
    "                \"min_temp\": data[\"min_temp\"],\n",
    "                \"max_temp\": data[\"max_temp\"],\n",
    "                \"text_description\": data[\"text_description\"]\n",
    "            }\n",
    "            \n",
    "            vectors_to_upsert.append({\n",
    "                \"id\": vector_id,\n",
    "                \"values\": embedding,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "        \n",
    "        # æ‰¹æ¬¡ä¸Šå‚³å‘é‡\n",
    "        batch_size = 100\n",
    "        for i in tqdm(range(0, len(vectors_to_upsert), batch_size), desc=\"ä¸Šå‚³å‘é‡\"):\n",
    "            batch = vectors_to_upsert[i:i + batch_size]\n",
    "            self.index.upsert(vectors=batch)\n",
    "        \n",
    "        print(f\"âœ… æˆåŠŸä¸Šå‚³ {len(vectors_to_upsert)} å€‹å‘é‡åˆ° Pinecone\")\n",
    "    \n",
    "    def query_weather(self, question: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        æŸ¥è©¢å¤©æ°£è³‡æ–™\n",
    "        \n",
    "        Args:\n",
    "            question: æŸ¥è©¢å•é¡Œ\n",
    "            top_k: è¿”å›çš„æœ€ç›¸é—œçµæœæ•¸é‡\n",
    "            \n",
    "        Returns:\n",
    "            æœ€ç›¸é—œçš„å¤©æ°£è³‡æ–™åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # ä½¿ç”¨ Pinecone Inference API ç”ŸæˆæŸ¥è©¢çš„åµŒå…¥å‘é‡\n",
    "            response = self.pc.inference.embed(\n",
    "                model=self.embedding_model,\n",
    "                inputs=[question],\n",
    "                parameters={\"input_type\": \"query\"}\n",
    "            )\n",
    "            query_embedding = response.data[0].values\n",
    "            \n",
    "            # åœ¨ Pinecone ä¸­æœå°‹\n",
    "            search_results = self.index.query(\n",
    "                vector=query_embedding,\n",
    "                top_k=top_k,\n",
    "                include_metadata=True\n",
    "            )\n",
    "            \n",
    "            # æ ¼å¼åŒ–çµæœ\n",
    "            results = []\n",
    "            for match in search_results.matches:\n",
    "                result = {\n",
    "                    \"score\": match.score,\n",
    "                    \"district\": match.metadata[\"district\"],\n",
    "                    \"time_range\": match.metadata[\"time_range\"],\n",
    "                    \"avg_temp\": match.metadata[\"avg_temp\"],\n",
    "                    \"min_temp\": match.metadata[\"min_temp\"],\n",
    "                    \"max_temp\": match.metadata[\"max_temp\"],\n",
    "                    \"text_description\": match.metadata[\"text_description\"]\n",
    "                }\n",
    "                results.append(result)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def interactive_query(self) -> None:\n",
    "        \"\"\"äº’å‹•å¼æŸ¥è©¢ä»‹é¢\"\"\"\n",
    "        print(\"\\nğŸ¯ å¤©æ°£æŸ¥è©¢ç³»çµ±å·²å•Ÿå‹•ï¼\")\n",
    "        print(\"æ‚¨å¯ä»¥è©¢å•é—œæ–¼è‡ºä¸­å¸‚å¤©æ°£çš„å•é¡Œï¼Œä¾‹å¦‚ï¼š\")\n",
    "        print(\"- è‡ºä¸­å¸‚ä¸­å€çš„æº«åº¦å¦‚ä½•ï¼Ÿ\")\n",
    "        print(\"- ä»Šå¤©è‡ºä¸­æœ€ç†±çš„åœ°å€æ˜¯å“ªè£¡ï¼Ÿ\")\n",
    "        print(\"- è‡ºä¸­å¸‚å“ªå€‹å€åŸŸæº«å·®æœ€å¤§ï¼Ÿ\")\n",
    "        print(\"è¼¸å…¥ 'quit' é€€å‡º\\n\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"ğŸ¤” è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ: \").strip()\n",
    "            \n",
    "            if question.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:\n",
    "                print(\"ğŸ‘‹ å†è¦‹ï¼\")\n",
    "                break\n",
    "            \n",
    "            if not question:\n",
    "                continue\n",
    "            \n",
    "            print(\"ğŸ” æ­£åœ¨æœå°‹ç›¸é—œè³‡æ–™...\")\n",
    "            results = self.query_weather(question)\n",
    "            \n",
    "            if not results:\n",
    "                print(\"âŒ æ²’æœ‰æ‰¾åˆ°ç›¸é—œè³‡æ–™\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nğŸ“Š æ‰¾åˆ° {len(results)} å€‹ç›¸é—œçµæœï¼š\\n\")\n",
    "            \n",
    "            for i, result in enumerate(results[:3], 1):  # åªé¡¯ç¤ºå‰3å€‹çµæœ\n",
    "                print(f\"{i}. {result['text_description']}\")\n",
    "                print(f\"   ç›¸ä¼¼åº¦: {result['score']:.3f}\")\n",
    "                print()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»ç¨‹å¼\"\"\"\n",
    "    # API é‡‘é‘°è¨­å®š\n",
    "    PINECONE_API_KEY = \"pcsk_7PG62n_6tsydkgfVzMfyxhgjP1GCg3DV6r7Qk3mFcT33aQg37ovghyFMNurdfWEA16i831\"\n",
    "    \n",
    "    # å¤©æ°£é å ±æ–‡ä»¶è·¯å¾‘\n",
    "    WEATHER_FILE_PATH = r\"E:\\python_project\\contest\\TGIS\\code\\weather_description.md\"\n",
    "    \n",
    "    print(\"ğŸŒ¤ï¸  è‡ºä¸­å¸‚å¤©æ°£é å ±å‘é‡åŒ–ç³»çµ±\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # åˆå§‹åŒ–å‘é‡åŒ–è™•ç†å™¨\n",
    "        vectorizer = WeatherVectorizer(PINECONE_API_KEY)\n",
    "        \n",
    "        # æª¢æŸ¥ç´¢å¼•ç‹€æ…‹\n",
    "        index_stats = vectorizer.index.describe_index_stats()\n",
    "        total_vectors = index_stats.total_vector_count\n",
    "        \n",
    "        if total_vectors > 0:\n",
    "            print(f\"ğŸ“Š ç´¢å¼•ä¸­å·²æœ‰ {total_vectors} å€‹å‘é‡\")\n",
    "            user_input = input(\"æ˜¯å¦è¦é‡æ–°è™•ç†è³‡æ–™ï¼Ÿ(y/N): \").strip().lower()\n",
    "            process_data = user_input in ['y', 'yes', 'æ˜¯']\n",
    "        else:\n",
    "            print(\"ğŸ“Š ç´¢å¼•ç‚ºç©ºï¼Œå°‡é–‹å§‹è™•ç†è³‡æ–™\")\n",
    "            process_data = True\n",
    "        \n",
    "        if process_data:\n",
    "            # æå–è‡ºä¸­å¸‚å¤©æ°£è³‡æ–™\n",
    "            weather_data = vectorizer.extract_taichung_data(WEATHER_FILE_PATH)\n",
    "            \n",
    "            if not weather_data:\n",
    "                print(\"âŒ æœªèƒ½æå–åˆ°æœ‰æ•ˆçš„å¤©æ°£è³‡æ–™\")\n",
    "                return\n",
    "            \n",
    "            # å‰µå»ºåµŒå…¥å‘é‡\n",
    "            texts = [data[\"text_description\"] for data in weather_data]\n",
    "            embeddings = vectorizer.create_embeddings(texts)\n",
    "            \n",
    "            # ä¸Šå‚³åˆ° Pinecone\n",
    "            vectorizer.upload_to_pinecone(weather_data, embeddings)\n",
    "        \n",
    "        # å•Ÿå‹•äº’å‹•å¼æŸ¥è©¢\n",
    "        vectorizer.interactive_query()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ç¨‹å¼åŸ·è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "water",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
